adam_beta1:
  default: '0.9'
  help: The beta1 parameter for the Adam optimizer.
  type: float
adam_beta2:
  default: '0.999'
  help: The beta2 parameter for the Adam optimizer.
  type: float
adam_epsilon:
  default: 1e-08
  help: Epsilon value for the Adam optimizer
  type: float
adam_weight_decay:
  default: 1e-2
  help: Weight decay to use.
  type: float
allow_tf32:
  default: 'False'
  help: Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training.
    For more information, see https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
  type: bool
cache_dir:
  default: None
  help: The directory where the downloaded models and datasets will be stored.
  type: str
center_crop:
  default: 'False'
  help: Whether to center crop the input images to the resolution. If not set, the
    images will be randomly cropped. The images will be resized to the resolution
    first before cropping.
  type: bool
checkpointing_steps:
  default: '500'
  help: Save a checkpoint of the training state every X updates. These checkpoints
    are only suitable for resuming training using `--resume_from_checkpoint`.
  type: int
checkpoints_total_limit:
  default: '5'
  help: Max number of checkpoints to store. Passed as `total_limit` to the `Accelerator`
    `ProjectConfiguration`. See Accelerator::save_state https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.save_state
    for more docs
  type: int
conditioning_dropout_prob:
  default: None
  help: ''
  type: float
dataloader_num_workers:
  default: '0'
  help: Number of subprocesses to use for data loading. 0 means that the data will
    be loaded in the main process.
  type: int
dataset_config_name:
  default: None
  help: The config of the Dataset, leave as None if there's only one config.
  type: str
dataset_name:
  default: None
  help: ''
  type: str
edit_prompt_column:
  default: '"edit_prompt"'
  help: The column of the dataset containing the edit instruction.
  type: str
edited_image_column:
  default: '"edited_image"'
  help: The column of the dataset containing the edited image.
  type: str
enable_xformers_memory_efficient_attention:
  default: 'False'
  help: Whether or not to use xformers.
  type: bool
gradient_accumulation_steps:
  default: '1'
  help: Number of updates steps to accumulate before performing a backward/update
    pass.
  type: int
gradient_checkpointing:
  default: 'False'
  help: Whether or not to use gradient checkpointing to save memory at the expense
    of slower backward pass.
  type: bool
hub_model_id:
  default: None
  help: The name of the repository to keep in sync with the local `output_dir`.
  type: str
hub_token:
  default: None
  help: The token to use to push to the Model Hub.
  type: str
learning_rate:
  default: 1e-4
  help: ''
  type: float
local_rank:
  default: '-1'
  help: 'For distributed training: local_rank'
  type: int
logging_dir:
  default: '"logs"'
  help: ''
  type: str
lora_alpha:
  default: '32'
  help: Lora alpha, only used if use_lora is True
  type: int
lora_bias:
  default: '"none"'
  help: Bias type for Lora. Can be 'none', 'all' or 'lora_only', only used if use_lora
    is True
  type: str
lora_dropout:
  default: '0.0'
  help: Lora dropout, only used if use_lora is True
  type: float
lora_r:
  default: '8'
  help: Lora rank, only used if use_lora is True
  type: int
lr_scheduler:
  default: '"constant"'
  help: The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts",
    "polynomial", "constant", "constant_with_warmup"]
  type: str
lr_warmup_steps:
  default: '500'
  help: Number of steps for the warmup in the lr scheduler.
  type: int
max_grad_norm:
  default: '1.0'
  help: Max gradient norm.
  type: float
max_train_samples:
  default: None
  help: For debugging purposes or quicker training, truncate the number of training
    examples to this value if set.
  type: int
max_train_steps:
  default: None
  help: Total number of training steps to perform.  If provided, overrides num_train_epochs.
  type: int
mixed_precision:
  default: '"fp16"'
  help: ''
  type: str
non_ema_revision:
  default: None
  help: Revision of pretrained non-ema model identifier. Must be a branch, tag or
    git identifier of the local or remote repository specified with --pretrained_model_name_or_path.
  type: str
num_train_epochs:
  default: null
  help: null
  type: int
num_validation_images:
  default: '4'
  help: Number of images that should be generated during validation with `validation_prompt`.
  type: int
original_image_column:
  default: '"input_image"'
  help: The column of the dataset containing the original image on which edits where
    made.
  type: str
output_dir:
  default: '"instruct-pix2pix-model"'
  help: The output directory where the model predictions and checkpoints will be written.
  type: str
pretrained_model_name_or_path:
  default: None
  help: Path to pretrained model or model identifier from huggingface.co/models.
  type: str
push_to_hub:
  default: 'False'
  help: Whether or not to push the model to the Hub.
  type: bool
random_flip:
  default: 'False'
  help: whether to randomly flip images horizontally
  type: bool
report_to:
  default: '"tensorboard"'
  help: The integration to report the results and logs to. Supported platforms are
    `"tensorboard"`
  type: str
resolution:
  default: '256'
  help: The resolution for input images, all the images in the train/validation dataset
    will be resized to this resolution
  type: int
resume_from_checkpoint:
  default: None
  help: Whether training should be resumed from a previous checkpoint. Use a path
    saved by `--checkpointing_steps`, or `"latest"` to automatically select the last
    available checkpoint.
  type: str
revision:
  default: None
  help: Revision of pretrained model identifier from huggingface.co/models.
  type: str
scale_lr:
  default: 'False'
  help: Scale the learning rate by the number of GPUs, gradient accumulation steps,
    and batch size.
  type: bool
seed:
  default: None
  help: A seed for reproducible training.
  type: int
train_batch_size:
  default: '16'
  help: ''
  type: int
train_data_dir:
  default: None
  help: A folder containing the training data. Folder contents must follow the structure
    described in https://huggingface.co/docs/datasets/image_dataset#imagefolder. In
    particular, a `metadata.jsonl` file must exist to provide the captions for the
    images. Ignored if `dataset_name` is specified.
  type: str
use_8bit_adam:
  default: 'False'
  help: Whether or not to use 8-bit Adam from bitsandbytes.
  type: bool
use_ema:
  default: 'False'
  help: Whether to use EMA model.
  type: bool
use_lora:
  default: 'False'
  help: Whether to use Lora for parameter efficient tuning
  type: bool
val_image_url:
  default: None
  help: ''
  type: str
validation_epochs:
  default: '1'
  help: 'Run fine-tuning validation every X epochs. The validation process consists
    of running the prompt `args.validation_prompt` multiple times: `args.num_validation_images`.'
  type: int
validation_prompt:
  default: None
  help: A prompt that is sampled during training for inference.
  type: str
